<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Are you kidding?</title>
    <link>https://nimisolo.github.io/post/index.xml</link>
    <description>Recent content in Posts on Are you kidding?</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Tue, 27 Jun 2017 21:58:50 +0800</lastBuildDate>
    <atom:link href="https://nimisolo.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>手撕intel-iommu之DMA remapping初始化</title>
      <link>https://nimisolo.github.io/post/vtd-dma-remapping-sw/</link>
      <pubDate>Tue, 27 Jun 2017 21:58:50 +0800</pubDate>
      
      <guid>https://nimisolo.github.io/post/vtd-dma-remapping-sw/</guid>
      <description>&lt;p&gt;我在DMA remapping相关的初始化函数中加入dump_stack()函数，以此来跟踪内核初始化过程中intel iommu之DMA remapping初始化流程。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;start_kernel
	mem_init
		pci_iommu_alloc
			detect_intel_iommu
				dmar_table_detect
				dmar_walk_dmar_table
					for (iter = start; iter &amp;lt; end; iter = next) {
						dmar_validate_one_drhd
					}
				x86_init.iommu.iommu_init = intel_iommu_init
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;font color=blue&gt;&lt;code&gt;detect_intel_iommu&lt;/code&gt;的作用是探测平台是否支持intel iommu功能&lt;/font&gt;，其主要步骤如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;调用 dmar_table_detect 在ACPI表中查找是否有DMAR表&lt;/li&gt;
&lt;li&gt;调用 dmar_walk_dmar_table 对DMAR表的每一项执行 validate_drhd_cb 中指定的操作。此处会对每一表通过 dmar_validate_one_drhd 判断是否是合法&lt;/li&gt;
&lt;li&gt;设置iommu_init钩子为 intel_iommu_init&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kernel_init
	kernel_init_freeable
		do_one_initcall
			pci_iommu_init
				iommu_init钩子(intel_iommu_init)
					iommu_init_mempool
					dmar_table_init
						if (dmar_table_initialized == 0) {
							parse_dmar_table
							dmar_table_initialized = 1
						}
					dmar_dev_scope_init
					dmar_init_reserved_ranges
						reserve_iova(IOAPIC_RANGE_START, IOAPIC_RANGE_END)
						for_each_pci_dev(pdev) {
							reserve_iova(r-&amp;gt;start, r-&amp;gt;end)
						}
					init_no_remapping_devices
					init_dmars
					dma_ops = &amp;amp;intel_dma_ops;
					bus_set_iommu
					intel_iommu_enabled = 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;font color=blue&gt;&lt;code&gt;intel_iommu_init&lt;/code&gt;对intel iommu进行全面的初始化&lt;/font&gt;，其主要步骤如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;调用 iommu_init_mempool 初始化3个kmem_cache：iova_cache（&amp;rdquo;iommu_iova&amp;rdquo;）、iommu_domain_cache（&amp;rdquo;iommu_domain&amp;rdquo;）、iommu_devinfo_cache（&amp;rdquo;iommu_devinfo&amp;rdquo;）&lt;/li&gt;
&lt;li&gt;调用 dmar_table_init &amp;ndash;&amp;gt; parse_dmar_table 对ACPI DMA Remapping相关的表（DRHD/RMRR/ATSR/RHSA/ANDD）进行解析&lt;font color=red&gt;具体每项如何解析未看&amp;hellip;vt-d spec ch8&lt;/font&gt;&lt;/li&gt;
&lt;li&gt;调用 dmar_dev_scope_init 将每个pci dev添加到所属的DMAR hardware unit中&lt;font color=red&gt;不明白&lt;/font&gt;&lt;/li&gt;
&lt;li&gt;调用 dmar_init_reserved_ranges 将MSI地址区间及所有pci dev的MMIO区间加入reserved_iova_list，这些区间不能被remapping（前者vt-d ch3.13有说明；&lt;font color=red&gt;后者是为何？近期看到社区有推使能pcie p2p支持的patch(&lt;a href=&#34;https://lists.01.org/pipermail/linux-nvdimm/2017-January/008395.html&#34;&gt;Enabling peer to peer device transactions for PCIe devices&lt;/a&gt;)，难道是软件还不支持？&lt;/font&gt;）&lt;/li&gt;
&lt;li&gt;调用 init_no_remapping_devices ：绝大多数gfx drivers不会调用standard PCI DMA APIs来分配DMA buffers，这与IOMMU有冲突。因此如果一个DMA remapping hardware unit中如果只有gdx devices，则根据cmdline配置来决定iommu是否需要将它们bypass掉。&lt;a href=&#34;https://lkml.org/lkml/2007/4/24/226&#34;&gt;原始patch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;init_dmars 对 intel_iommu 做详细的初始化设置（具体见下文分析）&lt;/li&gt;
&lt;li&gt;调用 bus_set_iommu 设置pci bus的iommu_ops钩子为intel_iommu_ops，并注册了一个bus notifier —— iommu_bus_notifier&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;init_dmars
	g_iommus = kcalloc(....)
	for_each_possible_cpu(cpu) {
		setup_timer(&amp;amp;dfd-&amp;gt;timer, flush_unmaps_timeout, cpu)
	}
	for_each_active_iommu(iommu, drhd) {
		g_iommus[iommu-&amp;gt;seq_id] = iommu
		intel_iommu_init_qi
			dmar_enable_qi
			iommu-&amp;gt;flush.flush_context = qi_flush_context
			iommu-&amp;gt;flush.flush_iotlb = qi_flush_iotlb
		iommu_init_domains
		iommu_alloc_root_entry
	}
	for_each_active_iommu(iommu, drhd) {
		iommu_set_root_entry
		iommu-&amp;gt;flush.flush_context
		iommu-&amp;gt;flush.flush_iotlb
	}
	if (iommu_identity_mapping) {
		si_domain_init
		iommu_prepare_static_identity_mapping
	}
	for_each_rmrr_units(rmrr) {
		for_each_active_dev_scope {
			iommu_prepare_rmrr_dev
		}
	}
	iommu_prepare_isa
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;分配用于存储所有intel_iommu的数组空间&lt;/li&gt;
&lt;li&gt;初始化per-cpu的deferred_flush_data对象，它在 IOTLB invalid 操作时被使用&lt;/li&gt;
&lt;li&gt;DMA Remapping转换过程中可能会有多种translation caches，在软件改变转换表时需要invalid相关old caches。vt-d中提供了两种invalid的方式：Register-based invalidation interface 和 Queued invalidation interface，如果需要支持irq remapping，则必须用后者，故我们分析后者（vt-d spec ch6.5.2）。&lt;br/&gt;对于平台上的每个active的iommu，通过 intel_iommu_init_qi 对其进行初始化设置：

&lt;ul&gt;
&lt;li&gt;分配相关数据结构，其中包括了一个作为Invalidation Queue的page&lt;/li&gt;
&lt;li&gt;将DMAR_IQT_REG（Invalidation Queue Tail Register）设置为0&lt;/li&gt;
&lt;li&gt;设置DMAR_IQA_REG（Invalidation Queue Address Register）：IQ的地址和大小&lt;/li&gt;
&lt;li&gt;设置DMAR_GCMD_REG（Global Command Register）使能QI功能&lt;/li&gt;
&lt;li&gt;等待DMAR_GSTS_REG（Global Status Register）的QIES置位，表示使能成功&lt;/li&gt;
&lt;li&gt;设置flush.flush_context和flush.flush_iotlb两个钩子&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;通过 iommu_init_domains 对intel_iommu数据结构中的domain_ids、domains域进行初始化&lt;/li&gt;
&lt;li&gt;通过 iommu_alloc_root_entry 分配一个用作iommu root entry的page，存储在iommu-&amp;gt;root_entry中&lt;/li&gt;
&lt;li&gt;通过 iommu_set_root_entry 设置root table地址：

&lt;ul&gt;
&lt;li&gt;设置DMAR_RTADDR_REG（Root Table Address Register），设置root table基地址&lt;/li&gt;
&lt;li&gt;写DMAR_GCMD_REG的SRTP位进行设置&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;当cmdline中有“iommu=pt”（表示只对直通设备做DMA Remapping）时iommu_pass_through会设为1，init_dmars中编会设置iommu_identity_mapping。&lt;br/&gt;调用 si_domain_init 对si_domain（statically identity mapping domain）进行初始化&lt;br/&gt;调用 iommu_prepare_static_identity_mapping 将每个pci dev与si_domain关联起来&lt;/li&gt;
&lt;li&gt;对于RMRR中每一项锁包含的每一个dev，调用 iommu_prepare_rmrr_dev &amp;ndash;&amp;gt; domain_prepare_identity_map 为其建立identity mapping&lt;/li&gt;
&lt;li&gt;调用 iommu_prepare_isa 为isa bridge建立identity mapping&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;注：最后两步骤中，在我的PC上并没有建立identity mapping，从结果可知：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[    0.995584] DMAR: Setting RMRR:
[    0.995585] DMAR: Ignoring identity map for HW passthrough device 0000:00:02.0 [0xc3800000 - 0xc7ffffff]
[    0.995585] DMAR: Ignoring identity map for HW passthrough device 0000:00:14.0 [0xc14c1000 - 0xc14e0fff]
[    0.995586] DMAR: Prepare 0-16MiB unity mapping for LPC
[    0.995586] DMAR: Ignoring identity map for HW passthrough device 0000:00:1f.0 [0x0 - 0xffffff]
[    0.995593] DMAR: Intel(R) Virtualization Technology for Directed I/O
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;domain_prepare_identity_map中有如下桥段：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;   2652         if (domain == si_domain &amp;amp;&amp;amp; hw_pass_through) {
   2653                 pr_warn(&amp;quot;Ignoring identity map for HW passthrough device %s [0x%Lx - 0x%Lx]\n&amp;quot;,
   2654                         dev_name(dev), start, end);
   2655                 return 0;
   2656         }
   2657
   2658         pr_info(&amp;quot;Setting identity map for device %s [0x%Lx - 0x%Lx]\n&amp;quot;,
   2659                 dev_name(dev), start, end);
   2660
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;并没有输出&amp;rdquo;Setting identity map for device&amp;hellip;&amp;ldquo;，所以我的PC此时全走了if块中、即直接return了。&lt;br/&gt;&lt;strong&gt;hw_pass_through&lt;/strong&gt;表示平台上的iommu硬件是否支持只对直通设备做remapping的功能。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>手撕intel-iommu之DMA remapping（硬件篇）</title>
      <link>https://nimisolo.github.io/post/vtd-dma-remapping/</link>
      <pubDate>Thu, 22 Jun 2017 20:18:38 +0800</pubDate>
      
      <guid>https://nimisolo.github.io/post/vtd-dma-remapping/</guid>
      <description>

&lt;p&gt;由于KVM虚拟机的设备直通方案目前基本都采用vfio方式，而vfio下层依赖于平台的iommu功能，因此我决定研究下intel iommu。计划根据Intel vt-d spec和Linux内核相关源码（主要在&lt;code&gt;drviers/iommu/&lt;/code&gt;下面）进行学习。&lt;/p&gt;

&lt;h2 id=&#34;font-color-red-dma-remapping-font&#34;&gt;&lt;font color=red&gt;DMA Remapping&lt;/font&gt;&lt;/h2&gt;

&lt;h3 id=&#34;font-color-red-types-of-dma-requests-font&#34;&gt;&lt;font color=red&gt;Types of DMA requests&lt;/font&gt;&lt;/h3&gt;

&lt;p&gt;Remapping硬件将来自于 集成在root-complex中 或 挂载在PCIE bus上的 设备的memory request分成两类：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Requests-without-PASID&lt;/strong&gt;：这是来自于endpoint devices的普通memory requests。它们一般指明了访问类型(read/write/atomics)、DMA目的地的地址与大小、源设备标识。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Requests-with-PASID&lt;/strong&gt;：这是来自于支持virtual memory capabilities的endpoint devices的memory requests。它们除了指明上述普通信息外，还带有附加信息：目标PASID、扩展属性等&amp;hellip;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;（注：PASID全名Process Address Space ID，来源于PCIE中的概念）&lt;/p&gt;

&lt;h3 id=&#34;font-color-red-domains-and-address-translation-font&#34;&gt;&lt;font color=red&gt;Domains and Address Translation&lt;/font&gt;&lt;/h3&gt;

&lt;p&gt;Domain是一个抽象的定义，表示平台上一个隔离的、从host physical memory中分配的子集。对于虚拟化来说，软件可以将每个虚拟机看作是一个单独的domain。&lt;/p&gt;

&lt;p&gt;Remapping硬件的作用是：在硬件进行进一步处理（例如：address decoding, snooping of processor caches, and/or forwarding to the memory controllers）之前，将memory request中的地址转换成host physical address。&lt;/p&gt;

&lt;h3 id=&#34;font-color-red-mapping-devices-to-domains-font&#34;&gt;&lt;font color=red&gt;Mapping Devices to Domains&lt;/font&gt;&lt;/h3&gt;

&lt;h4 id=&#34;font-color-red-source-identifier-font&#34;&gt;&lt;font color=red&gt;Source Identifier&lt;/font&gt;&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;source-id&lt;/strong&gt;：用于标识I/O transaction的发起者。
对于PCIE设备，其source-id位于PCI-Express transaction layer header中，由Bus/Device/Function组成。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://nimisolo.github.io/iommu-source-id.JPG&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;font-color-red-root-entry-extended-root-entry-font&#34;&gt;&lt;font color=red&gt;Root-Entry &amp;amp; Extended-Root-Entry&lt;/font&gt;&lt;/h4&gt;

&lt;p&gt;Root-table是devices mapping的最顶层结构。它的起始地址由&lt;strong&gt;Root Table Address Register&lt;/strong&gt;指定，共计4-KB大小，由256个root-entries组成（可以算出：每个root-entry有16个字节空间）。每个root-entry对应一个bus number，所以source-id中的bus number在mapping过程中会被当做root-table中的index来使用。&lt;/p&gt;

&lt;h5 id=&#34;font-color-red-root-table-address-register-font&#34;&gt;&lt;font color=red&gt;Root Table Address Register&lt;/font&gt;&lt;/h5&gt;

&lt;p&gt;我们首先来看看Root Table Address Register的详细描述：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://nimisolo.github.io/iommu-rtar.JPG&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://nimisolo.github.io/iommu-rtar-2.JPG&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;可以看到：有两种形式的table，根据RTT bit进行控制。下面就依次来看这两种table。&lt;/p&gt;

&lt;h5 id=&#34;font-color-red-regular-root-table-entry-font&#34;&gt;&lt;font color=red&gt;regular Root-table entry&lt;/font&gt;&lt;/h5&gt;

&lt;p&gt;&lt;img src=&#34;https://nimisolo.github.io/iommu-rte.JPG&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://nimisolo.github.io/iommu-rte-2.JPG&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h5 id=&#34;font-color-red-extended-root-table-entry-font&#34;&gt;&lt;font color=red&gt;Extended Root-table entry&lt;/font&gt;&lt;/h5&gt;

&lt;p&gt;在支持Extended-Context-Support (ECS=1 in Extended Capability Register)的硬件上，如果RTADDR_REG的RTT=1，则它指向的是一个extended root-table。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://nimisolo.github.io/iommu-extended-rte.JPG&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://nimisolo.github.io/iommu-extended-rte-2.JPG&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;综上，又会涉及到两种context entry，下面依次来看。&lt;/p&gt;

&lt;h5 id=&#34;font-color-red-regular-context-entry-font&#34;&gt;&lt;font color=red&gt;regular Context-Entry&lt;/font&gt;&lt;/h5&gt;

&lt;p&gt;一个context-entry用于将一个bus上的一个特定I/O device映射到其所属的domain中，也就是指向该domain的地址翻译结构。&lt;/p&gt;

&lt;p&gt;source-id中的低8位（device#和function#）用来作为设备在context-table中的index使用。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://nimisolo.github.io/iommu-map.JPG&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;font color=blue&gt;Context-entries只能支持requests-without-PASID&lt;/font&gt;，它详细结构如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://nimisolo.github.io/iommu-ce.JPG&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h5 id=&#34;font-color-red-extended-context-entry-font&#34;&gt;&lt;font color=red&gt;Extended-Context-Entry&lt;/font&gt;&lt;/h5&gt;

&lt;p&gt;一个Extended root entry可以同时索引一个lower-context-table和一个upper-context-table，这两者都是4-KB大小、包含128个extended-context-entries。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;lower-context-table 对应了特定bus（root entry索引）上device#为0-15的PCI设备&lt;/li&gt;
&lt;li&gt;upper-context-table 对应了特定bus（root entry索引）上device#为16-31的PCI设备&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://nimisolo.github.io/iommu-map2.JPG&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Extended-context-entries既支持requests-without-PASID，也支持requests-with-PASID。对于前者，entry结构与上述regular context entry一致；对于后者，其结构如下所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://nimisolo.github.io/iommu-ece.JPG&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;font-color-red-first-level-translation-font&#34;&gt;&lt;font color=red&gt;First-level Translation&lt;/font&gt;&lt;/h3&gt;

&lt;p&gt;Extended-Context-Entry可以被配置为通过First-level Translation来翻译requests-with-PASID。这种配置下Extended-Context-Entry包含了指向PASID-table的指针和大小，而requests-with-PASID中的PASID-number作为PASID-table中的offset来索引一个PASID-entry。在PASID-entry中包含了相应进程地址空间的first-level translation structure的指针。&lt;/p&gt;

&lt;h3 id=&#34;font-color-red-second-level-translation-font&#34;&gt;&lt;font color=red&gt;Second-level Translation&lt;/font&gt;&lt;/h3&gt;

&lt;p&gt;Extended-Context-Entry可以被配置为使用second-level translation。这种配置下，Extended-Context-Entry中包含了指向second-level translation structure的指针。&lt;/p&gt;

&lt;p&gt;second-level translation可以用来转换requests-without-PASID，也可以在nested translation过程中用来转换requests-with-PASID的first-level translation。（&lt;font color=blue&gt;这种使用方式还不太明白，或许和嵌套虚拟化相关，L1虚拟机向L2虚拟机呈现vt-d dma remapping功能时或许会用到&lt;/font&gt;）&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Linux Concurrency Managed Workqueue分析</title>
      <link>https://nimisolo.github.io/post/workqueue/</link>
      <pubDate>Wed, 22 Mar 2017 20:38:22 +0800</pubDate>
      
      <guid>https://nimisolo.github.io/post/workqueue/</guid>
      <description>

&lt;h3 id=&#34;概述&#34;&gt;概述&lt;/h3&gt;

&lt;h3 id=&#34;核心数据结构&#34;&gt;核心数据结构&lt;/h3&gt;

&lt;h3 id=&#34;框架分析&#34;&gt;框架分析&lt;/h3&gt;

&lt;h4 id=&#34;workqueue子系统初始化&#34;&gt;workqueue子系统初始化&lt;/h4&gt;

&lt;h5 id=&#34;workqueue-init-early&#34;&gt;workqueue_init_early&lt;/h5&gt;

&lt;p&gt;此函数对workqueue子系统做早期初始化。它会建立某些数据结构及创建系统的workquues，其他模块的早期初始化代码在这之后便可以queue/cancel work items了，但是这些work items只有在相关worker kthread建立（会在&lt;code&gt;workqueue_init&lt;/code&gt;中做）之后才能够得到运行。&lt;/p&gt;

&lt;p&gt;此函数中最核心一步是&lt;font color=red&gt;为每个cpu初始化了两个普通worker pools（一个nice=0的、一个nice=-20的）&lt;/font&gt;，这些worker pool都存在per-cpu变量中：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/* the per-cpu worker pools */
static DEFINE_PER_CPU_SHARED_ALIGNED(struct worker_pool [NR_STD_WORKER_POOLS], cpu_worker_pools);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;font color=red&gt;实际上，无论是普通worker-pool还是unbound/ordered worker-pool，都有低优先级和高优先级两种。&lt;/font&gt;&lt;/p&gt;

&lt;p&gt;因为普通worker pool是per-cpu的，其cpumask也就只含有相应cpu，在创建work线程时会用它来指定线程的cpu亲和性，所以&lt;font color=red&gt;对于普通work poll来说，其work线程是只能在相应的cpu上运行，不能migrate&lt;/font&gt;&lt;/p&gt;

&lt;h5 id=&#34;workqueue-init&#34;&gt;workqueue_init&lt;/h5&gt;

&lt;p&gt;此函数是workqueue子系统初始化的第二步（也是最后一步）。&lt;/p&gt;

&lt;p&gt;它首先会为每个cpu的每个普通worker pool设置好其所在的node，存储在&lt;code&gt;pool-&amp;gt;node&lt;/code&gt;中；&lt;/p&gt;

&lt;p&gt;然后通过&lt;code&gt;create_worker&lt;/code&gt;为每个普通worker pool创建一个worker（包括一个工作线程），worker创建初始化完成后会通过&lt;code&gt;worker_enter_idle&lt;/code&gt;进入WORKER_IDLE状态；&lt;/p&gt;

&lt;p&gt;除了普通worker pool之外，系统中此时可能已有ubound worker pool（&lt;code&gt;workqueue_init_early&lt;/code&gt;之后其他子系统早期代码中可能会创建），此时也需要通过&lt;code&gt;create_worker&lt;/code&gt;为它们创建worker。&lt;/p&gt;

&lt;h4 id=&#34;worker创建&#34;&gt;worker创建&lt;/h4&gt;

&lt;h5 id=&#34;create-worker&#34;&gt;create_worker&lt;/h5&gt;

&lt;p&gt;此函数用于为@pool创建一个worker。主要步骤如下：&lt;/p&gt;

&lt;p&gt;1）通过&lt;code&gt;alloc_worker&lt;/code&gt;分配一个worker对象;&lt;/p&gt;

&lt;p&gt;2）调用&lt;code&gt;kthread_create_on_node&lt;/code&gt;函数创建该worker的工作线程，工作线程的执行体是&lt;strong&gt;&lt;code&gt;worker_thread&lt;/code&gt;&lt;/strong&gt;;&lt;/p&gt;

&lt;p&gt;&lt;font color=red&gt;&lt;strong&gt;注意：&lt;/strong&gt;&lt;/font&gt;&lt;code&gt;kthread_create_on_node&lt;/code&gt;会通过&lt;code&gt;kthreadd_task&lt;/code&gt;线程在指定node上（具体哪个cpu上不可测）创建一个内核线程。&lt;/p&gt;

&lt;p&gt;3）设置工作线程的优先级（&lt;code&gt;set_user_nice&lt;/code&gt;）;&lt;/p&gt;

&lt;p&gt;4）将工作线程绑定至指定的cpumask（&lt;code&gt;kthread_bind_mask&lt;/code&gt;）;&lt;/p&gt;

&lt;p&gt;&lt;font color=red&gt;&lt;strong&gt;注意：&lt;/strong&gt;&lt;/font&gt;&lt;code&gt;kthread_bind_mask&lt;/code&gt;中会无条件将线程的allowed_cpumask设置为新的cpumask，但即使线程当前所处的cpu不在新的cpumask中也不会做迁移操作。&lt;/p&gt;

&lt;p&gt;5）通过&lt;code&gt;worker_attach_to_pool&lt;/code&gt;函数将此worker与@pool关联起来，加入到@pool的workers链表中;&lt;/p&gt;

&lt;p&gt;&lt;font color=red&gt;&lt;strong&gt;注意：&lt;/strong&gt;&lt;/font&gt;&lt;code&gt;worker_attach_to_pool&lt;/code&gt;中调用了&lt;code&gt;set_cpus_allowed_ptr&lt;/code&gt;函数，但是对于worker kthread来说调用它是没有实际意义的，因为它会检查allowed_cpumask是否与new_cpumask相等，如果相等的话什么也不干，&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;int set_cpus_allowed_ptr(struct task_struct *p, const struct cpumask *new_mask)
{
	return __set_cpus_allowed_ptr(p, new_mask, false);
}

static int __set_cpus_allowed_ptr(struct task_struct *p,
				  const struct cpumask *new_mask, bool check)
{
	......
	if (cpumask_equal(&amp;amp;p-&amp;gt;cpus_allowed, new_mask))
	goto out;
	......
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;但这里为什么需要调用呢？&lt;code&gt;worker_attach_to_pool&lt;/code&gt;在rescuer_thread也会调用。一个workqueue可以对应多个worker pool，但是只有一个rescuer线程，当其某个worker pool需要“急救”时，rescuer_thread通过·将自己“派到一线”去救援。&lt;/p&gt;

&lt;p&gt;对于worker thread来说，只有等到被调度运行的时候才有机会被迁移到合适的cpu上运行（try_to_wake_up&amp;ndash;&amp;gt;select_task_rq）。&lt;/p&gt;

&lt;h3 id=&#34;font-color-red-关键问题-font&#34;&gt;&lt;font color=red&gt;关键问题&lt;/font&gt;&lt;/h3&gt;

&lt;h4 id=&#34;worker的亲和性能改变吗&#34;&gt;worker的亲和性能改变吗&lt;/h4&gt;

&lt;p&gt;worker线程在创建中会通过&lt;code&gt;kthread_bind_mask&lt;/code&gt;设置它的allowed_cpumask，并且会设置PF_NO_SETAFFINITY标记。&lt;/p&gt;

&lt;p&gt;改变亲和性的内核函数是&lt;code&gt;__set_cpus_allowed_ptr&lt;/code&gt;,&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;static int __set_cpus_allowed_ptr(struct task_struct *p,
				  const struct cpumask *new_mask, bool check)
{
	......
	if (check &amp;amp;&amp;amp; (p-&amp;gt;flags &amp;amp; PF_NO_SETAFFINITY)) {
		ret = -EINVAL;
		goto out;
	}
	......
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果&lt;code&gt;check&lt;/code&gt;为真，则不能设置带&lt;code&gt;PF_NO_SETAFFINITY&lt;/code&gt;标记的线程的亲和性。&lt;/p&gt;

&lt;p&gt;&lt;font color=red&gt;特殊的，&lt;/font&gt;rescuer线程也是worker，它在运行过程中会调用&lt;code&gt;worker_attach_to_pool&lt;/code&gt;根据具体情况而调整自己的亲和性，而：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;static void worker_attach_to_pool(struct worker *worker,
				   struct worker_pool *pool)
{
	......
	set_cpus_allowed_ptr(worker-&amp;gt;task, pool-&amp;gt;attrs-&amp;gt;cpumask);
	......
}

int set_cpus_allowed_ptr(struct task_struct *p, const struct cpumask *new_mask)
{
	return __set_cpus_allowed_ptr(p, new_mask, false);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到传入的check=false，意味着即使带PF_NO_SETAFFINITY标记也要修改，所以，&lt;font color=red&gt;rescuer线程在运行过程中会动态调整自己的亲和性，除此之外内核不会主动调整其他worker线程的亲和性&lt;/font&gt;。&lt;/p&gt;

&lt;p&gt;用户态程序（例如taskset）可通过&lt;code&gt;sched_setaffinity&lt;/code&gt;来改变线程的亲和性，但是它会首先判断线程是否有PF_NO_SETAFFINITY标记，有的话则不能修改亲和性，&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;long sched_setaffinity(pid_t pid, const struct cpumask *in_mask)
{
	......
	if (p-&amp;gt;flags &amp;amp; PF_NO_SETAFFINITY) {
		retval = -EINVAL;
		goto out_put_task;
	}
	......
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;所以，&lt;font color=red&gt;用户态程序不能修改worker线程的亲和性。&lt;/font&gt;&lt;/p&gt;

&lt;h4 id=&#34;worker何时唤醒-休眠&#34;&gt;worker何时唤醒/休眠&lt;/h4&gt;

&lt;p&gt;首先说说“休眠”，也就是让worker进入&lt;code&gt;WORKER_IDLE&lt;/code&gt;状态。进入休眠状态的函数是&lt;code&gt;worker_enter_idle&lt;/code&gt;，根据其被调用地方，我们可以得到如下结论：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;worker刚创建完成后会进入休眠状态，因此此时在&lt;code&gt;worker_thread&lt;/code&gt;中会进入sleep分支，然后进入休眠：&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;static int worker_thread(void *__worker)
{
	......
recheck:
	/* no more worker necessary? */
	if (!need_more_worker(pool))
		goto sleep;
	......
sleep:
	worker_enter_idle(worker);
	__set_current_state(TASK_INTERRUPTIBLE);
	spin_unlock_irq(&amp;amp;pool-&amp;gt;lock);
	schedule();
	goto woke_up;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;rescuer作用及运行时间&#34;&gt;rescuer作用及运行时间&lt;/h4&gt;

&lt;h4 id=&#34;worker何种情况下会扩建&#34;&gt;worker何种情况下会扩建&lt;/h4&gt;

&lt;h4 id=&#34;worker何时销毁&#34;&gt;worker何时销毁&lt;/h4&gt;

&lt;h4 id=&#34;work太多了如何并发处理&#34;&gt;work太多了如何并发处理&lt;/h4&gt;

&lt;h4 id=&#34;timers的作用&#34;&gt;timers的作用&lt;/h4&gt;
</description>
    </item>
    
    <item>
      <title>Linux内核idle进程分析</title>
      <link>https://nimisolo.github.io/post/mwait_idle/</link>
      <pubDate>Sun, 19 Feb 2017 08:42:58 +0800</pubDate>
      
      <guid>https://nimisolo.github.io/post/mwait_idle/</guid>
      <description>

&lt;h3 id=&#34;写作背景&#34;&gt;写作背景&lt;/h3&gt;

&lt;p&gt;在当前KVM实现中，当vcpu中执行HLT指令时会发生VMexit，KVM模块中会进入&lt;code&gt;kvm_vcpu_block&lt;/code&gt;函数，从前此函数会快速将vcpu线程schedule out，但在配置了halt_poll_ns的情况下，当条件满足时会适当的busy loop一段时间，代码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;void kvm_vcpu_block(struct kvm_vcpu *vcpu)
{
	...
		do {
			/*
			 * This sets KVM_REQ_UNHALT if an interrupt
			 * arrives.
			 */
			if (kvm_vcpu_check_block(vcpu) &amp;lt; 0) {
				++vcpu-&amp;gt;stat.halt_successful_poll;
				if (!vcpu_valid_wakeup(vcpu))
					++vcpu-&amp;gt;stat.halt_poll_invalid;
				goto out;
			}
			cur = ktime_get();
		} while (single_task_running() &amp;amp;&amp;amp; ktime_before(cur, stop));	
	...
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当vcpu此时不再是halt状态 或 当前pcpu上不止一个可运行task 或 busy loop时间到期 这三者任一满足时将退出busy loop，在busy loop过程中CPU使用率肯定会冲高到100%。可是我们不想将这段时间被TOP工具即pmcint工具（利用PMU）统计到，前者可以通过修改内核cputime.c中的几个统计代码来实现，但是后者的话稍微有点麻烦。pmcint工具利用了PMU硬件打点的原理，我们没法修改统计，最后想到的办法是改造此busy loop，在其中进入C1 state，这样的话PMU计数器会暂停，从而使得pmcint暂停打点，这样既确保了vcpu性能，又无法使用户看到很高的占用率。&lt;/p&gt;

&lt;p&gt;细想一下，此处busy loop的好处是啥？无非就是在条件满足的情况下不要将vcpu线程切换出去，否则来回切换势必使得vcpu线程被唤醒不够及时，导致虚拟机性能受影响。这样来说的话，我们在busy loop中调用HLT/MWAIT也能起到一样的效果：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;如果使用MWAIT，我们可以用MONITOR监控让busy loop结束的变量，这样当需要结束时MWAIT会被broken，从而与此前流程一样。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;如果使用HLT，则我们需要根据stop设置一个backend hrtimer，保证我们肯定能够从HLT中出来。（但是，这个方式肯定不如上述代码的实现，因为HLT的broken事件中不包括内存监控，它没法及时监控到nr_running的变化，这可能会导致调度不及时）&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;font color=red&gt;&lt;strong&gt;[更新]&lt;/strong&gt;突然想到一个此处换做HLT/MWAIT的一个优势：在开启硬件超线程的host上，如果同一core上的一个硬件线程的vcpu1进行halt_poll_ns而另外个硬件线程上的vcpu2在运行业务，那么这种做法或许比社区的好，因为vcpu1 exit执行HLT/MWAIT指令后，该core的硬件资源此时能够完全给另一个硬件线程独占，该线程性能会因资源独占而变好。&lt;/font&gt;&lt;/p&gt;

&lt;h3 id=&#34;cpu-cstates&#34;&gt;CPU Cstates&lt;/h3&gt;

&lt;p&gt;Cstates是ACPI规范中引入的，具体的可以看acpi spec，下面内容摘自&lt;a href=&#34;http://www.expreview.com/25426.html&#34;&gt;Haswell芯光大道之六：C-States十种状态解析&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&#34;三种常见cpu工作状态简介&#34;&gt;三种常见CPU工作状态简介&lt;/h4&gt;

&lt;p&gt;常见的CPU工作状态包含S-States、C-States和P-States三种，其中&lt;strong&gt;S-States（Sleeping states）指系统睡眠状态&lt;/strong&gt;，&lt;strong&gt;C-States（CPU Power states）指CPU电源状态&lt;/strong&gt;，而&lt;strong&gt;P-States（CPU Performance states）则指CPU性能状态&lt;/strong&gt;。当然除了这三种外，还有G-States（全局状态）和D-States（设备状态）。&lt;/p&gt;

&lt;p&gt;S-States很好理解，就是你手动点击“睡眠”，或者达到一定的待机时间（根据系统电源管理设置而定）进入睡眠状态，S0就是指正常运作。而C-States和P-States看起来也很类似，都会调节处理器的核心电压、电流以及频率，因此经常被混淆。其实他们的区别也是很明显的，不过我们首先要梳理一下上面这三种状态的关系。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://nimisolo.github.io/intel-power-state.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;S-States中的S0指非睡眠状态，包含了系统正常运作状态以及待机状态，这意味着只有在S0状态下，C-States才会存在。同样的，C0代表正常工作状态，而P-States正是处理器正常运作时的状态，所以P-States只存在于C0状态下。&lt;/p&gt;

&lt;p&gt;简单来说，P-States是根据系统的负载情况调节处理器核心电压和频率，处理器仍在运作当中；而C-States则是改变处理器各个部分的状态，包括核心、缓存、总线以及各种后来集成进来的模块，此时处理器应该是工作或待机状态。我们日常使用电脑的时候，系统就是频繁地在这些状态下切换，以达到提高续航和降低功耗的目的。&lt;/p&gt;

&lt;h4 id=&#34;c-states各个状态介绍&#34;&gt;C-States各个状态介绍&lt;/h4&gt;

&lt;p&gt;目前C-States有以下这11个状态：&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;C0&lt;/strong&gt;：正常运行模式，我们正常操作电脑时均处于C0状态。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;C1/C1E&lt;/strong&gt;：挂起/待机状态，通过软件（一般发送HLT命令）停止处理器内部时钟。增强版的C1E支持降低倍频和电压。使用CPU-Z会观察到频率、电压下降，表示系统进入了这个状态，当外部总线传来请求时就会暂时离开C1/C1E状态（只需10纳秒），处理完后会恢复。这个状态仅对硬件延迟有要求，不过目前的硬件一般都没问题。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;C2/C2E&lt;/strong&gt;：和C1/C1E类似，但C2/C2E状态通过硬件进入，而且唤醒需要100纳秒以上。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;C3&lt;/strong&gt;：深度睡眠，内部时钟同样会被停止，总线频率会被锁定，多核心系统下缓存数据保留，并暂停写入操作，无法响应外部总线的重要请求。进入C3状态的前提是硬件支持并已进入C2模式。唤醒时间在50微秒左右。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;C4&lt;/strong&gt;：更深度睡眠，在C3状态的基础上通过将电压降至1.0V以下与减少L2缓存的数据存储以降低功耗。需要进入C3后才能进入C4，另外唤醒时间不超过1秒。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;C4E&lt;/strong&gt;：同样需要进入C4状态，并且L2缓存数据被减为零。唤醒时间至少需要200微秒。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;C6&lt;/strong&gt;：深度节能，处理器可实时清除L1缓存内所有数据，在保存处理器微架构状态下，关掉内核及L2缓存，芯片组会继续为I/O提供内存交换动作。对各个核心电源进行更智能的管理，电压降至C4的一半。不过唤醒时间要比C4长50%。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;C7&lt;/strong&gt;：更深度节能，在C6的基础上增加了清空部分或者全部L3缓存。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;C8&lt;/strong&gt;：L3缓存、系统助手（也就是以前北桥整合到CPU中的部分）和IO供电都被关闭，外部VR模块电压降至1.2V。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;C9&lt;/strong&gt;：VR模块电压接近0V。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;C10&lt;/strong&gt;：关闭VR模块。（不确定）&lt;/p&gt;

&lt;h3 id=&#34;monitor-mwait指令&#34;&gt;MONITOR/MWAIT指令&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;MONITOR指令&lt;/strong&gt;能够监控一个Linear Address（监控的范围通常是cache line size），当向该地址写入操作时会被检测到。The MONITOR instruction arms the address monitoring hardware using the address specified in EAX. The address range that the monitoring hardware will check for store operations can be determined by the CPUID instruction. The monitoring hardware will detect stores to an address within the address range and triggers the monitor hardware when the write is detected. The state of the monitor hardware is used by the MWAIT instruction.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;MWAIT指令&lt;/strong&gt; provides hints to allow the processor to enter an implementation-dependent optimized state. There are two principal targeted usages: address-range monitor and advanced power management.
A store to the address range armed by the MONITOR instruction, an interrupt, an NMI or SMI, a debug exception, a machine check exception, the BINIT# signal, the INIT# signal, or the RESET# signal will exit the implementation-dependent-optimized state.
In addition, an external interrupt causes the processor to exit the implementation-dependent-optimized state either (1) if the interrupt would be delivered to software (e.g., as it would be if HLT had been executed instead of MWAIT); or (2) if ECX[0] = 1. Software can execute MWAIT with ECX[0] = 1 only if CPUID.05H:ECX[bit 1] = 1. (Implementation-specific conditions may result in an interrupt causing the processor to exit the implementation-dependent-optimized state even if interrupts are masked and ECX[0] = 0.)&lt;/p&gt;

&lt;p&gt;&lt;font color=red&gt; 注意： &lt;/font&gt;上面提到了MWAIT的牛逼之处（相比HLT）：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;可以进入指定的C-state&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;唤醒事件中包括了向MONITOR地址进行写入操作&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;即使interrupt处于disabled状态，也有可能（看CPU是否支持）可以被interrupt唤醒&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;会不会有安全问题&#34;&gt;会不会有安全问题&lt;/h4&gt;

&lt;p&gt;&lt;font color=red&gt;&lt;strong&gt;问题一：&lt;/strong&gt;&lt;/font&gt;虚拟化下，如果guest kernel中执行了MONITOR gva指令，此时会不会影响到host中正处于mwait状态的cpu？例如：无论是某个vcpu STORE了该gva、或者某个pcpu STORE了某hva（而此hva的数值与该gva相等），都将引起mwait BROKEN?&lt;/p&gt;

&lt;p&gt;&lt;font color=blue&gt;&lt;strong&gt;解决：&lt;/strong&gt;&lt;/font&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;intel SDM vol3 26.3.3 / 27.5.6 说，VMentry/VMexit时会将可能有影响的（或正在生效的?? may be in effect, 如何翻译更好??）所有“address-range monitoring”清除。（题外话：什么叫“可能有影响”？&lt;strong&gt;[a]&lt;/strong&gt;我觉得都是“可能有影响的”，因为该address-range是个virtual address，对于64位host+64位guest的线性地址空间大小是一样的，硬件上无法区分，所以应该是都会清除。&lt;strong&gt;[b]&lt;/strong&gt;但是这样的话又不对了，假如pcpu监控了hva1，然后它进入了guest，这时需要将hva1清除，当VMexit后并没有提到恢复对hva1的监控，那么此pcpu上后续STORE hva1不就不能被监控了吗？&lt;strong&gt;[c]&lt;/strong&gt;除非有种可能，address-range由至少二元组确定，即virtual address + cpu mode，但是这样的话SDM何不直接描述为“guest或non-root的address-range清除”即可？&lt;font color=red&gt;哎&amp;hellip;暂时不懂&lt;/font&gt;）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;font color=red&gt;&lt;strong&gt;[2017/03/22更新]&lt;/strong&gt; &lt;a href=&#34;http://marc.info/?l=kvm&amp;amp;m=149006091218214&amp;amp;w=4&#34;&gt;和KVM co-maintainer Radim做了些讨论&lt;/a&gt;，最终认为这是没有问题的，Intel spec vol2中对MWAIT有这样的描述：&lt;/p&gt;

&lt;p&gt;If the preceding MONITOR instruction did not successfully arm an address range or if the MONITOR instruction has not been executed prior to executing MWAIT, then the processor will not enter the implementation-dependent-optimized state. Execution will resume at the instruction following the MWAIT.&lt;/p&gt;

&lt;p&gt;也就是说，如果guest在monitor/mwait之间发生了vmexit，则monitor address-range会被clear掉，但再次vmentry时MWAIT行为和NOP一样（原因或许是：此时MWAIT执行时发现monitor address-range不存在，就当做&amp;rdquo; did not successfully arm an address range&amp;rdquo;，所以&amp;rdquo;resume at the instruction following the MWAIT&amp;rdquo;）&lt;/p&gt;

&lt;p&gt;讨论相关记录：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt; 2) According to the &amp;quot;Intel sdm vol3 ch26.3.3 &amp;amp; ch27.5.6&amp;quot;, I think MONITOR in
&amp;gt;&amp;gt; guest mode can&#39;t work as perfect as in host sometimes.
&amp;gt;&amp;gt; For example, a vcpu MONITOR a address and then MWAIT, if a external-intr(suppose
&amp;gt;&amp;gt; this intr won&#39;t cause to inject any virtual events ) cause VMEXIT, the monitor
&amp;gt;&amp;gt; address will be cleaned, so the MWAIT won&#39;t be waken up by a store operation to
&amp;gt;&amp;gt; the monitored address any more.
&amp;gt; 
&amp;gt; It&#39;s not as perfect, but should not cause a bug (well, there is a
&amp;gt; discussion with suspicious MWAIT behavior :]).
&amp;gt; MWAIT on all Intels I tested would just behave as a nop if exit happened
&amp;gt; between MONITOR and MWAIT, like it does if you skip the MONITOR (MWAIT
&amp;gt; instruction desciption):
&amp;gt; 
&amp;gt;   If the preceding MONITOR instruction did not successfully arm an
&amp;gt;   address range or if the MONITOR instruction has not been executed
&amp;gt;   prior to executing MWAIT, then the processor will not enter the
&amp;gt;   implementation-dependent-optimized state. Execution will resume at the
&amp;gt;   instruction following the MWAIT.
&amp;gt; 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/font&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;intel SDM vol3 25.1.3 说，如果VM-execution control的“MONITOR exiting”为1，则MONITOR指令会导致VMexit。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在KVM中，在&lt;code&gt;setup_vmcs_config&lt;/code&gt;中“MONITOR exiting”是会强制为1的，所以vcpu只要执行该指令就会引起VMexit，退出处理&lt;code&gt;handle_monitor&lt;/code&gt;中直接跳过该指令、把它当做NOP来处理。&lt;/p&gt;

&lt;p&gt;&lt;font color=red&gt;&lt;strong&gt;[2017/03/22更新]问题二：&lt;/strong&gt;&lt;/font&gt; Intel sdm vol3 ch25.3说在某些情况下，guest可正常执行MWAIT指令，正常包括进入各种deeper C-state？如果包括的话那么这就有蛋疼的事儿了：某些deeper sleep会clear缓存（L1、L2、L3都有可能），而vmx又没有能够限制guest max-cstates的方法，vcpu可以影响到其他pcpu cache，这太可怕了&amp;hellip;.&lt;/p&gt;

&lt;p&gt;这个观点Radim很是认同，但我还需要做个实验，确定guest能够进入各种deeper sleep。&lt;/p&gt;

&lt;p&gt;讨论相关记录：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt; 1) As &amp;quot;Intel sdm vol3 ch25.3&amp;quot; says, MWAIT operates normally (I think includes
&amp;gt;&amp;gt; entering deeper sleep) under certain conditions.
&amp;gt;&amp;gt; Some deeper sleep modes(such as C4E/C6/C7) will clear the L1/L2/L3 cache.
&amp;gt;&amp;gt; This is insecurity if we don&#39;t take other protective measures(such as limit the
&amp;gt;&amp;gt; guest&#39;s max-cstate, it&#39;s fortunately that power subsystem isn&#39;t supported by
&amp;gt;&amp;gt; QEMU, but we should be careful for some special-purpose in case). While HLT in
&amp;gt;&amp;gt; guest mode can&#39;t cause hardware into sleep.
&amp;gt; 
&amp;gt; Good point.  I&#39;m not aware of any VMX capabilities to prevent deeper
&amp;gt; C-states, so we&#39;d always hope that guests obey provided information.
&amp;gt; 
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;linux-idle进程&#34;&gt;Linux idle进程&lt;/h3&gt;

&lt;p&gt;idle进程的执行体是&lt;code&gt;do_idle&lt;/code&gt;函数，此篇中我们关注的片段如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;void cpu_startup_entry(enum cpuhp_state state)
{
	......
	while (1)
		do_idle();
}

/*
 * Generic idle loop implementation
 *
 * Called with polling cleared.
 */
static void do_idle(void)
{
	......

	while (!need_resched()) {
		......
		if (cpu_idle_force_poll || tick_check_broadcast_expired())
			cpu_idle_poll();
		else
			cpuidle_idle_call();
		......
	}

	......
	sched_ttwu_pending(); /* 唤醒其他需要唤醒的任务 */
	schedule_preempt_disabled(); /* 通过schedule()主动切换到其他runnable任务 */
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果配置了idle=poll，则会走&lt;code&gt;cpu_idle_poll&lt;/code&gt;函数，它实际上就是在那里一直轮询检查&lt;code&gt;当前是否需要进行任务调度&lt;/code&gt;。
没配置的话则会走&lt;code&gt;cpuidle_idle_call&lt;/code&gt;函数：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
/**
 * cpuidle_idle_call - the main idle function
 *
 * NOTE: no locks or semaphores should be used here
 *
 * On archs that support TIF_POLLING_NRFLAG, is called with polling
 * set, and it returns with polling set.  If it ever stops polling, it
 * must clear the polling bit.
 */
static void cpuidle_idle_call(void)
{
	struct cpuidle_device *dev = cpuidle_get_device();
	struct cpuidle_driver *drv = cpuidle_get_cpu_driver(dev);
	int next_state, entered_state;

	......

	if (cpuidle_not_available(drv, dev)) {
		default_idle_call();
		goto exit_idle;
	}

	/*
	 * Suspend-to-idle (&amp;quot;freeze&amp;quot;) is a system state in which all user space
	 * has been frozen, all I/O devices have been suspended and the only
	 * activity happens here and in iterrupts (if any).  In that case bypass
	 * the cpuidle governor and go stratight for the deepest idle state
	 * available.  Possibly also suspend the local tick and the entire
	 * timekeeping to prevent timer interrupts from kicking us out of idle
	 * until a proper wakeup interrupt happens.
	 */

	if (idle_should_freeze() || dev-&amp;gt;use_deepest_state) {
		if (idle_should_freeze()) {
			entered_state = cpuidle_enter_freeze(drv, dev);
			if (entered_state &amp;gt; 0) {
				local_irq_enable();
				goto exit_idle;
			}
		}

		next_state = cpuidle_find_deepest_state(drv, dev);
		call_cpuidle(drv, dev, next_state);
	} else {
		/*
		 * Ask the cpuidle framework to choose a convenient idle state.
		 */
		next_state = cpuidle_select(drv, dev);
		entered_state = call_cpuidle(drv, dev, next_state);
		/*
		 * Give the governor an opportunity to reflect on the outcome
		 */
		cpuidle_reflect(dev, entered_state);
	}
	......
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果系统中没有高级电源管理模块（例如acpi_driver或者intel_driver），则会调用&lt;code&gt;default_idle_call&lt;/code&gt;函数，对于X86来说它会执行HLT指令。
如果有的话，则会计算出一个将要进入的C-state，然后通过&lt;code&gt;call_cpuidle&lt;/code&gt;函数进入。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
static int call_cpuidle(struct cpuidle_driver *drv, struct cpuidle_device *dev,
		      int next_state)
{
	......

	/*
	 * Enter the idle state previously returned by the governor decision.
	 * This function will block until an interrupt occurs and will take
	 * care of re-enabling the local interrupts
	 */
	return cpuidle_enter(drv, dev, next_state);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里&lt;code&gt;cpuidle_enter&lt;/code&gt; &amp;ndash;&amp;gt; &lt;code&gt;cpuidle_enter_state&lt;/code&gt; &amp;ndash;&amp;gt; &lt;code&gt;entered_state = target_state-&amp;gt;enter(dev, drv, index);&lt;/code&gt;，最后通过&lt;code&gt;-&amp;gt;enter&lt;/code&gt;回调函数进入相应C-state。
对于intel_idle驱动来说，这个回调函数是&lt;code&gt;intel_idle&lt;/code&gt;函数，它最终会调用&lt;code&gt;mwait_idle_with_hints&lt;/code&gt;函数来执行MWAIT指令。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/*
 * This uses new MONITOR/MWAIT instructions on P4 processors with PNI,
 * which can obviate IPI to trigger checking of need_resched.
 * We execute MONITOR against need_resched and enter optimized wait state
 * through MWAIT. Whenever someone changes need_resched, we would be woken
 * up from MWAIT (without an IPI).
 *
 * New with Core Duo processors, MWAIT can take some hints based on CPU
 * capability.
 */
static inline void mwait_idle_with_hints(unsigned long eax, unsigned long ecx)
{
	if (static_cpu_has_bug(X86_BUG_MONITOR) || !current_set_polling_and_test()) {
		if (static_cpu_has_bug(X86_BUG_CLFLUSH_MONITOR)) {
			mb();
			clflush((void *)&amp;amp;current_thread_info()-&amp;gt;flags);
			mb();
		}

		__monitor((void *)&amp;amp;current_thread_info()-&amp;gt;flags, 0, 0);
		if (!need_resched())
			__mwait(eax, ecx);
	}
	current_clr_polling();
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;首先会通过MONITOR指令监控idle进程的flags，然后通过MWAIT进入相应C-state。&lt;/p&gt;

&lt;p&gt;&lt;font color=red&gt;这里监控flags的原因很简单：当其他cpu唤醒了此cpu上某个任务的时候，（可参见&lt;code&gt;ttwu_queue&lt;/code&gt;函数）要么通过RES IPI、要么其他cpu直接将该任务放在此cpu的运行队列上，前者因为是个IPI中断，肯定会第一时间唤醒此cpu；对于后者，过程中有一步会将idle进程（如果此cpu当前运行的是idle）的flags置上NEED_RESCHED标记，由于此cpuMONITOR了flags，所以此cpu也能第一时刻就唤醒。&lt;/font&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ARM host/guest使用哪种timer</title>
      <link>https://nimisolo.github.io/post/arm-host/guest%E4%BD%BF%E7%94%A8%E5%93%AA%E7%A7%8Dtimer/</link>
      <pubDate>Sun, 05 Feb 2017 16:45:06 +0800</pubDate>
      
      <guid>https://nimisolo.github.io/post/arm-host/guest%E4%BD%BF%E7%94%A8%E5%93%AA%E7%A7%8Dtimer/</guid>
      <description>&lt;p&gt;ARM Generic Timer支持好几种timer，此前听说ARM linux host使用pyhsical timer，而guest使用virtual timer。&lt;font color=red&gt;一直不清楚ARM linux中是怎么探测和设置它的，今晚看了下有所眉目了。&lt;/font&gt;&lt;/p&gt;

&lt;p&gt;首先来看个全局变量：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;static enum ppi_nr arch_timer_uses_ppi = VIRT_PPI;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;arch_timer_uses_ppi&lt;/code&gt;表示使用的timer类型，默认是virtual timer。&lt;/p&gt;

&lt;p&gt;在&lt;code&gt;arch_timer_init&lt;/code&gt;中根据情况会对此作出调整：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;	if (is_hyp_mode_available() || !arch_timer_ppi[VIRT_PPI]) {
		bool has_ppi;

		if (is_kernel_in_hyp_mode()) {
			arch_timer_uses_ppi = HYP_PPI;
			has_ppi = !!arch_timer_ppi[HYP_PPI];
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到，对于host来说，会调整成physical timer。&lt;/p&gt;

&lt;p&gt;在&lt;code&gt;__arch_timer_setup&lt;/code&gt;中会对所使用的clocksource做设置：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;		switch (arch_timer_uses_ppi) {
		case VIRT_PPI:
			clk-&amp;gt;set_state_shutdown = arch_timer_shutdown_virt;
			clk-&amp;gt;set_state_oneshot_stopped = arch_timer_shutdown_virt;
			clk-&amp;gt;set_next_event = arch_timer_set_next_event_virt;
			break;
		case PHYS_SECURE_PPI:
		case PHYS_NONSECURE_PPI:
		case HYP_PPI:
			clk-&amp;gt;set_state_shutdown = arch_timer_shutdown_phys;
			clk-&amp;gt;set_state_oneshot_stopped = arch_timer_shutdown_phys;
			clk-&amp;gt;set_next_event = arch_timer_set_next_event_phys;
			break;
		default:
			BUG();
		}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;针对不同的timer（physical或virtual），设置了不同的钩子处理函数。这样一来，以后对clock进行操作的时候就各走各的路了，从而区分开来了。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>关于periodic-mode timer的问题</title>
      <link>https://nimisolo.github.io/post/%E5%85%B3%E4%BA%8Eperiodic-mode/</link>
      <pubDate>Sun, 05 Feb 2017 15:54:39 +0800</pubDate>
      
      <guid>https://nimisolo.github.io/post/%E5%85%B3%E4%BA%8Eperiodic-mode/</guid>
      <description>&lt;p&gt;今天和晓建讨论一个方案时，他说“即使定时器是periodic mode，当加入hrtimer时也有可能会去设置硬件计数器（tmict/tsc_deadline）的值”。
咦？！和我以前的认识不一样啊，根据以前的掌握：periodic mode情况下新加hrtimer时不会设置硬件计数器了，及时设置计数器是oneshot mode的做法啊！
于是重新翻了下相关代码，证明&lt;strong&gt;&lt;font color=red&gt;晓建理解错了、我的观点是正确的&lt;/font&gt;&lt;/strong&gt;。请看下面分析。&lt;/p&gt;

&lt;p&gt;我们以新加一个定时器的操作入手：&lt;code&gt;hrtimer_start --&amp;gt; hrtimer_start_range_ns
&lt;/code&gt;，在后者中有这么一段：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;	leftmost = enqueue_hrtimer(timer, new_base);
	if (!leftmost)
		goto unlock;

	if (!hrtimer_is_hres_active(timer)) {
		/*
		 * Kick to reschedule the next tick to handle the new timer
		 * on dynticks target.
		 */
		if (new_base-&amp;gt;cpu_base-&amp;gt;nohz_active)
			wake_up_nohz_cpu(new_base-&amp;gt;cpu_base-&amp;gt;cpu);
	} else {
		hrtimer_reprogram(timer, new_base);
	}
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;通过&lt;code&gt;enqueue_hrtimer&lt;/code&gt;将（新添加的）hrtimer插入到当前的timerqueue（其实是一个rb-tree）中。如果该hrtimer是“将会最早到期”的，则它会放在rb-tree的最左端，即leftrmost为1；否则leftmost为0。&lt;/li&gt;
&lt;li&gt;如果不是最左端，则直接返回。这是因为：此函数是perdioc mode和oneshot mode公用的，如果此定时器将最早到期，oneshot mode会根据起到期时间重设硬件计数器。&lt;/li&gt;
&lt;li&gt;如果没有启动高精度定时器（&lt;code&gt;hrtimer_is_hres_active&lt;/code&gt;返回false），则不会重设硬件计数器；否则（oneshot mode）就通过&lt;code&gt;hrtimer_reprogram&lt;/code&gt;重设。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;现在需要弄清楚的就是：&lt;code&gt;hrtimer_is_hres_active&lt;/code&gt;什么情况下返回true。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;static inline int hrtimer_is_hres_active(struct hrtimer *timer)
{
	return timer-&amp;gt;base-&amp;gt;cpu_base-&amp;gt;hres_active;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实际上就是看&lt;code&gt;hres_active&lt;/code&gt;是否为1。因此下面搜索下，看什么情况下会设置它为1。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;static void hrtimer_switch_to_hres(void)
{
	struct hrtimer_cpu_base *base = this_cpu_ptr(&amp;amp;hrtimer_bases);

	if (tick_init_highres()) {
		printk(KERN_WARNING &amp;quot;Could not switch to high resolution &amp;quot;
				    &amp;quot;mode on CPU %d\n&amp;quot;, base-&amp;gt;cpu);
		return;
	}
	base-&amp;gt;hres_active = 1;
	......
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当&lt;code&gt;tick_init_highres&lt;/code&gt;返回false时，才会设置&lt;code&gt;hres_active&lt;/code&gt;为1。看名字，该函数的作用是初始化高精度定时器（即oneshot mode），深入该函数，可以发现：如果成功切换成oneshot，则会返回0。
好，到这里其实我们已经弄清楚了：
&lt;font color=red&gt;
+ 高精度highres其实是指onshot-mode。
+ periodic mode下，即使新加一个leftmost的hrtimer也不会重设硬件计数器。只有onshot mode下才会。
&lt;/font&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;更进一步&lt;/strong&gt;，当hrtimer到期之后，是如何被处理的？
&lt;strong&gt;对于periodic-mode模式：&lt;/strong&gt;&lt;code&gt;tick_handle_periodic --&amp;gt; tick_periodic --&amp;gt; update_process_times --&amp;gt; run_local_timers --&amp;gt; hrtimer_run_queues --&amp;gt; __hrtimer_run_queues&lt;/code&gt;
&lt;strong&gt;对于oneshot-mode模式：&lt;/strong&gt;&lt;code&gt;hrtimer_interrupt --&amp;gt; __hrtimer_run_queues&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;处理超时的hrtimer是最终都是在&lt;strong&gt;&lt;code&gt;__hrtimer_run_queues&lt;/code&gt;&lt;/strong&gt;函数中完成，&lt;font color=red&gt;它会（根据当前时间）遍历所有已经超时的hrtimer，然后处理它们&lt;/font&gt;。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;再进一步&lt;/strong&gt;，为啥periodic-mode精度比oneshot-mode低低低？？
+ 因为periodic-mode下时钟中断触发时间是周期性的，也就是说在一个周期内到期的hrtimer只有在周期结束（时钟终端触发）时才能获得处理，显然实时性较差；并且如果一个周期内很多hrtimer到期，则依次处理它们，那么排在靠后的hrtimer实际上延迟就更大了。
+ oneshot-mode能够精确的根据leftmost hrtimer的到期时间来精确设置计数器，这样更加实时；并且根据此原理，一般多个hrtimer同时到期的情况应该比periodic-mode的少，则“依次处理”不会太长，这一点也比periodic-mode情况要好。&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>